---
title: "My title"
subtitle: "My subtitle if needed"
author: 
  - Denise Chang
thanks: "Code and data are available at: [https://github.com/DeniseChang9/Emergency_service_demands.git](https://github.com/DeniseChang9/Emergency_service_demands.git)."
date: today
date-format: long
abstract: "First sentence. Second sentence. Third sentence. Fourth sentence."
format: pdf
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(openxlsx)
library(readxl)
library(arrow)
library(here)
library(knitr)
library(modelsummary)
library(patchwork)
```

# Introduction

"In an emergency, seconds count." [@city_of_toronto_public_safety_alerts]. Emergency medical services (EMS) are essential to public health by providing care during life-threatening situations. However, Toronto’s paramedic services are struggling to meet demand. Toronto Auditor General’s Office reports that there were over 1,200 episodes in 2023 where no ambulances were available. [@toronto_auditor_paramedic_services]. To echo this report, this paper examines EMS demand using Toronto Paramedic Services’ incident data from Open Data Toronto.

In this analysis, I am interested in EMS demand against time-based factors in Toronto from 2017 to 2022. I explore trends in paramedic services, such as dispatch time, incident type, and number of units dispatched per incident to identify periods of high-volume demand and low availability of resources. I find that ... [ADD A FEW RESULTS HERE (or in a different paragraph??)]

The remainder of this paper is organized as follows. @sec-data discusses the data source and an overview of the studied variables. @sec-model constructs a model that predicts a shortage in paramedic resources based on time factors. @sec-results presents the results and findings of the exploration of the data. @sec-discussion discusses implications, limitations, and suggestions for future research. [TELEGRAPH APPENDIX HERE]

# Data {#sec-data}

## Overview
The dataset used for this analysis is titled "Paramedic Services Incident Data" and is published by Toronto Paramedic Services [@data_parmedic_source]. For this paper, the dataset is retrieved from the City of Toronto Open Data Portal.

The statistical programming language R [@R] is used to process, manage and visualize the data. Specifically, statistical libraries such as `opendatatoronto` [@opendatatoronto], `openxlsx` [@openxlsx] and `janitor` [@janitor] are used to simulate, download and clean the raw data. Libraries like `arrow` [@arrow] and `readxl` [@readxl] were used to save and read datasets. Other libraries like `knitr` [@knitr], `here` [@here] are used to load and to render tables. The library `tidyverse` [@tidyverse] is useful throughout the entire data manipulation process. 

The initial dataset features data on paramedic dispatch time, type of incident, priority level of each incident, number of paramedic units arrived at scene and forward sortation area of the incident. These features are annually refreshed on Open Data Toronto by Toronto Paramedic Services.

The data used for this paper was retrieved on November 25, 2024 and was last refreshed on October 5, 2023. 

## Measurements

The incident data is saved in different files according to the year of incident. The variables and measurement method of the variables are preserved throughout the recorded years. 

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: tbl-raw_data
#| tbl-cap: First Ten Rows of the Paramedic Services Incident Data from 2017

# read raw data
raw_2017 <- read_xlsx(here::here("data/01-raw_data/paramedic_services_2017.xlsx"))

# visualize the first 10 rows of the data
raw_2017 |>
  select(Dispatch_Time, Incident_Type, Priority_Number, Units_Arrived_At_Scene, Forward_Sortation_Area) |>
  slice(1:10) |>
  kable(col.names = c("Dispatch Time", "Incident Type", "Priority Number", "Units Arrived", "Location"))
```

@tbl-raw_data is a sample of the 10 first entries in the incident data in 2017. Each row in the dataset represents a unique incident. "Dispatch Time" is the precise time when the first paramedic unit was assigned to an incident. This is measured in year, month, date, hour, minutes, and seconds. "Incident Type" is the category assigned to the incident by the dispatcher based on the information provided by the 9.1.1. caller(s). The possible categories of incident are medical emergencies, emergency transfers, fire-related incidents and motor vehicle accidents. "Priority number" represents the urgency of an incident with 1 being the most urgent and 5 being the least. The priority number is measured by the Medical Priority Dispatch System (MPDS) based on the information provided by the 9.1.1. caller(s). "Units Arrived" is the total number of paramedics that arrived on the scene of incident. This is measured by counting the number of different paramedic units who were dispatched and responded to the incident. Units part of this count do not have to be simultaneously present, and a unit who leaves the scene and comes back afterwards is considered 1 count. 
"Location" is the general location of the incident based on Postal Code Forward Sortation Areas. This is determined as the first three characters of the postal code of the incident location. 

## Variables of Interest

In the analysis dataset, most variables were constructed using the variables in the raw dataset. From the original dataset, "Dispatch Time" was broken down into four separate variables in the analysis dataset: "Year", "Month", "Day of the Week" and "Hour of the Day". The aim for this construction is to isolate the effect of each temporal factor on the paramedic incident request paramedic response. While holding the time variables and the "Incident Type" variable constant, two more variables were constructed from the "Units Arrived" variable from the raw dataset: "Average Units Arrived" and "Number of Incidents".

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: tbl-avg_units
#| tbl-cap: Ten First Rows of the Analysis Data on Paramedic Services Incident Data from 2017 to 2022

# read raw data
clean_data <- read_parquet(here::here("data/02-analysis_data/analysis_data.parquet"))

# visualize the first 10 rows of the data
clean_data |>
  slice(1:10) |>
  kable(col.names = c("Year", "Month", "Day of the Week", "Hour of the Day", "Incident Type", "Average Units Arrived", "Number of Incidents" ))

```
@tbl-avg_units is a sample of the analysis dataset used in this paper. The variables of interest of the analysis dataset are:

- Number of incidents:This variable represents the total count of emergency incidents recorded for each unique combination of time and incident-type factors. This is the outcome variable in the model, since it reflects the demandfor paramedic services.

- Average number of units arrived: This variable reflects the average number of paramedic units dispatched and arrived at the scene for a given combination of incident type, time, and date. This is another outcome variable , since it provides insight into resource allocation.

- Incident Type: This categorical variable describes the nature of each emergency, such as medical emergencies, fire-related incidents, motor vehicle accidents, or emergency transfers. It captures the diversity of incidents and is useful for exploring differences in ressource needs

- Year: The year the incident occurred, capturing long-term trends in incident frequency.

- Month: The month of occurrence, which higlights seasonal trends

- Day of Week: The day on which the incident took place, accounting for patterns related to weekdays versus weekends.

- Hour: The hour of the day, capturing diurnal patterns in emergency service demand.

# Model {#sec-model}

The goal of our modelling strategy is twofold. Firstly,...

Here we briefly describe the Bayesian analysis model used to investigate... Background details and diagnostics are included in [Appendix -@sec-model-details].

## Model set-up
We run the model in R [@R] using the `rstanarm` package [@rstanarm]. Default priors from `rstanarm` are applied.
The estimating equating is as follows: 

Define $y_i$ as the number of incidents observed for a specific combination of factors. Then, $\mu_i$ is the expected number of incidents based on temporal and incident-specific predictors.  

\begin{align}  
y_i | \mu_i, \sigma &\sim \mbox{Normal}(\mu_i, \sigma) \\  
\mu_i &= \alpha + \beta_{\text{year}} \cdot \text{year}_i + \sum_{k=1}^{11} \beta_{\text{month}, k} \cdot \text{month}_{ik} + \sum_{j=1}^{6} \beta_{\text{day}, j} \cdot \text{day}_{ij} \\
&\quad + \beta_{\text{hour}} \cdot \text{hour}_i + \sum_{t=1}^{3} \beta_{\text{incident}, t} \cdot \text{incident}_{it} + \beta_{\text{units}} \cdot \text{avg\_units}_i \\  
\alpha &\sim \mbox{Normal}(0, 2.5) \\  
\beta &\sim \mbox{Normal}(0, 2.5) \\  
\sigma &\sim \mbox{Exponential}(1)  
\end{align}  
In this model, we define the variables as follows:

- $y_i$ is the number of incidents observed for a specific combination of factors

- $\mu_i$ is the expected number of incidents based on predictors

- $\alpha$ is the intercept term, representing the baseline number of incidents.

- $\beta_{\text{year}}$ is the coefficient for the year variable.

- $\beta_{\text{month}}$ is the set of coefficients for the month of the year.

- $\beta_{\text{day}}$ is the set of coefficients for the day of the week.

- $\beta_{\text{hour}}$ is the coefficient for the hour of the day.

- $\beta_{\text{incident}}$ is the set of coefficients for the incident types.

- $\beta_{\text{units}}$ is the coefficient for the average number of units arriving on scene

- $\sigma$ is the standard deviation of the residuals, representing unexplained variability in incident counts.

### Model justification
For this analysis, a Bayesian Generalized Linear Model (GLM) was chosen to predict incident call counts as a function of temporal factors, incident type, and resource allocation. This approach provides the flexibility to model log-transformed continuous outcomes and includes prior beliefs about the relationships between predictors and the outcome. The Bayesian framework quantifies uncertainty in parameter estimates and predictions, which is useful in our context where variability and uncertainty are prevalent. This model is appropriate given the observed overdispersion in the count data.

In Bayesian modeling, priors were defined to reflect plausible values. A $\mbox{Normal}(0, 10)$ prior was used for the intercept ($\alpha$) to allow a wide range of baseline incident counts across predictor combinations. Predictor coefficients ($\beta$) were assigned a $\mbox{Normal}(0, 2.5)$ prior to accommodate moderate effect sizes without being too restrictive. The residual standard deviation ($\sigma$) was given an $\mbox{Exponential}(1)$ prior to emphasize smaller residual errors. These priors avoid constraints that are too narrow, reducing bias in the estimates

This model relies on several assumptions. First, it assumes a linear relationship between predictors and the log-transformed outcome variable, which simplifies interpretation. The model also assumes residuals follow a Gaussian distribution, consistent with the log-transformation of the outcome variable. Despite its strengths, the model has limitations. It does not explicitly model nonlinear relationships or interactions,like those between temporal and incident type variables, which could capture more complex dynamics. Finally, the Gaussian residual assumption may be inappropriate in certain cases of extreme overdispersion.

Several alternative models were considered. Poisson regression, a natural choice for count data, was notr chosen due to overdispersion in the outcome variable. Negative binomial regression was also considered, but does not incorporate prior beliefs or uncertainties as naturally as Bayesian methods.

Ultimately, the Bayesian Gaussian GLM was selected since it can handle continuous log-transformed data and include prior beliefs while still being digesteable.Through Bayesian inference, the model quantifies uncertainty and provides a probabilistic framework ideal for emergency response planning. This approach ensures actionable predictions for resource allocation decisions.

# Results {#sec-results}
Our results are summarized in @tbl-model.

```{r}
#| message: false
#| warning: false
#| echo: false
#| label: tbl-model
#| tbl-cap: Model Summary of Incident Count

# read model
model <-
  readRDS(file = here::here("models/incident_model.rds"))

summary(model)
```

The Bayesian GLM model, based on 38,494 observations and 24 predictors, has a posterior predictive mean (mean_PPD) of 44.4, which aligns with observed incident counts. This shows that the model captures variability.

Temporal factors play a role in determining incident volume. The coefficient for hour (mean = 1.2) shows predictable variations in demand throughout the day, with highly precise estimates (sd = 0.0). Monthly and daily trends are captured through polynomial terms, reflecting the nonlinear nature of incident occurrences across different times of the year and week.

The analysis suggests the dominance of medical incidents in driving overall incident counts. The coefficient for medical calls (mean = 126.4) is much higher than the coefficient for fire-related incidents (mean = -6.6) and the coefficient for motor vehicle accidents (mean = 0.2). This shows that medical emergencies is the primary source of paramedic service demand, requiring special attention in operational planning.

The positive relationship between the average number of units arriving and incident counts (mean = 1.3) suggests that resource allocation is another determining factor. There is a correlation where increased resource availability may correspond to higher incident volumes. This reflects areas of concentrated demand and hints at resource allocation strategies.

## Hour of the Day on Paramedic Incident Demand and Reponse
```{r}
#| message: false
#| warning: false
#| echo: false
#| label: tbl-clock_plot
#| tbl-cap: Paramedic Incident Counts and Number of Units Arrived on Scene by Hour of the Day.

# Filter relevant columns
filtered_data <- clean_data %>%
  select(hour, count, avg_units_arrived)

# Summarize counts and average units by hour
hourly_summary <- filtered_data %>%
  group_by(hour) %>%
  summarize(
    total_count = sum(count),
    avg_units = mean(avg_units_arrived)
  )

# Clock plot for incident count
count_plot <- ggplot(hourly_summary, aes(x = hour, y = total_count)) +
  geom_col(fill = "steelblue", color = "black") +
  scale_x_continuous(breaks = 0:23, limits = c(0, 23)) +
  coord_polar(start = 0) +
  theme_minimal() +
  labs(
    title = "Incident Counts by Hour",
    x = "Hour (Clock Position)",
    y = "Number of Incidents"
  )

# Clock plot for average units arrived
units_plot <- ggplot(hourly_summary, aes(x = hour, y = avg_units)) +
  geom_col(fill = "coral", color = "black") +
  scale_x_continuous(breaks = 0:23, limits = c(0, 23)) +
  coord_polar(start = 0) +
  theme_minimal() +
  labs(
    title = "Average Units Arrived by Hour",
    x = "Hour (Clock Position)",
    y = "Avg Units Arrived"
  )

# Combine the plots side by side
combined_plot <- count_plot + units_plot

# Display the plots
invisible(print(combined_plot))
```

# Discussion {#sec-discussion}

## First discussion point {#sec-first-point}

If my paper were 10 pages, then should be be at least 2.5 pages. The discussion is a chance to show off what you know and what you learnt from all this. 

## Second discussion point

Please don't use these as sub-heading labels - change them to be what your point actually is.

## Third discussion point

## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

\appendix

# Appendix {-}


# Additional data details

# Model details {#sec-model-details}

## Posterior predictive check

In @fig-ppcheckandposteriorvsprior-1 we implement a posterior predictive check. This shows...

In @fig-ppcheckandposteriorvsprior-2 we compare the posterior with the prior. This shows... 

```{r}
#| eval: false
#| echo: false
#| message: false
#| warning: false
#| label: fig-ppcheckandposteriorvsprior
#| layout-ncol: 2
#| fig-cap: "Examining how the model fits, and is affected by, the data"
#| fig-subcap: ["Posterior prediction check", "Comparing the posterior with the prior"]

pp_check(first_model) +
  theme_classic() +
  theme(legend.position = "bottom")

posterior_vs_prior(first_model) +
  theme_minimal() +
  scale_color_brewer(palette = "Set1") +
  theme(legend.position = "bottom") +
  coord_flip()
```

## Diagnostics

@fig-stanareyouokay-1 is a trace plot. It shows... This suggests...

@fig-stanareyouokay-2 is a Rhat plot. It shows... This suggests...

```{r}
#| echo: false
#| eval: false
#| message: false
#| warning: false
#| label: fig-stanareyouokay
#| fig-cap: "Checking the convergence of the MCMC algorithm"
#| fig-subcap: ["Trace plot", "Rhat plot"]
#| layout-ncol: 2

plot(first_model, "trace")

plot(first_model, "rhat")
```



\newpage


# References


